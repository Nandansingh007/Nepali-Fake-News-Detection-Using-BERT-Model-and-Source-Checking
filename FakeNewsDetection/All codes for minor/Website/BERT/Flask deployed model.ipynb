{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a07191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import tensorflow_text as text\n",
    "from transformers import TFBertModel\n",
    "from ScrapeSearchEngine.SearchEngine import Google\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c439120a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "#Loading trained moddel\n",
    "loaded_model = tf.keras.models.load_model(\"model2.h5\",custom_objects={'KerasLayer':hub.KerasLayer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8156e39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing text\n",
    "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3\")\n",
    "# encoder_inputs = preprocessor(text_input)\n",
    "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/4\",trainable=False)\n",
    "\n",
    "def get_sentence_embeding(sentences):\n",
    "  preprocessed_text = bert_preprocess(sentences)\n",
    "  return bert_encoder(preprocessed_text)['pooled_output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a17d13d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing stop words\n",
    "import re\n",
    "\n",
    "with open('stopwords_new.txt', 'r',  encoding='utf-8') as file:\n",
    "    stop = file.read()\n",
    "    \n",
    "def preprocessing(text):\n",
    "    \n",
    "    #punctuation removal\n",
    "    text_punctuation = re.sub(r'[।?:;\\'\",.\\n&—‘’“”!()-]', '', text)\n",
    "    \n",
    "    \n",
    "    stopwords = []\n",
    "    for word in stop.split():\n",
    "        stopwords.append(word)\n",
    "        \n",
    "   \n",
    "    \n",
    "    text1 = text_punctuation\n",
    "    \n",
    "\n",
    "    str_temp = \"\"\n",
    "    for words in text1.split():\n",
    "      # checking if the word is not in the stopword file\n",
    "      if str(words) not in stopwords:\n",
    "        # adding the words that is not in stopword file to the str_temp \n",
    "        str_temp += words\n",
    "        str_temp += \" \"\n",
    "\n",
    "    #result after removing stopwords\n",
    "    return str_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a4fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "INFO:werkzeug:127.0.0.1 - - [18/May/2023 14:59:03] \"GET / HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [18/May/2023 14:59:03] \"GET /static/photos/image2.jpg HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [18/May/2023 14:59:03] \"GET /static/photos/image1.jpg HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [18/May/2023 14:59:03] \"GET /static/photos/fakenews.jpg HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [18/May/2023 14:59:05] \"GET /check_news HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [18/May/2023 14:59:05] \"\u001b[36mGET /static/photos/fakenews.jpg HTTP/1.1\u001b[0m\" 304 -\n",
      "INFO:werkzeug:127.0.0.1 - - [18/May/2023 14:59:05] \"GET /static/photos/image1.jpg HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [18/May/2023 14:59:05] \"GET /static/photos/image2.jpg HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [18/May/2023 14:59:05] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
      "INFO:werkzeug:127.0.0.1 - - [18/May/2023 14:59:20] \"POST /check HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [18/May/2023 14:59:20] \"GET /static/photos/image1.jpg HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [18/May/2023 14:59:20] \"GET /static/photos/image2.jpg HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [18/May/2023 14:59:20] \"\u001b[36mGET /static/photos/fakenews.jpg HTTP/1.1\u001b[0m\" 304 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar titles:  []\n",
      "Trusted websites:  []\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from flask import Flask, request, render_template, session\n",
    "from urllib.parse import urlparse\n",
    "import sys\n",
    "from flask import make_response\n",
    "from flask import jsonify\n",
    "from newspaper import Article\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "news_ss = []\n",
    "Prediction = ''\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/home', methods=['GET', 'POST'])\n",
    "def predicting_news():\n",
    "    #submitting news\n",
    "    if request.form.get('submit'):\n",
    "      url = request.form.get('url')       \n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    text = ''\n",
    "    for p in soup.find_all('p'):\n",
    "        text += p.text\n",
    "    words = text.split()\n",
    "    if len(words) > 600:\n",
    "        words = words[:600]\n",
    "    text = \" \".join(words)\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    title = article.title\n",
    "    publish_date = article.publish_date\n",
    "    authors = article.authors\n",
    "    Titles = 'Titles'\n",
    "    Description = 'Description'\n",
    "    Date = 'Date'\n",
    "    Author = 'Author'\n",
    "    return render_template('index.html',title = title,Titles=Titles,text = text,Description = Description,Date = Date,publish_date = publish_date,authors = authors, Author = Author)\n",
    "\n",
    "\n",
    "\n",
    "@app.route('/check_news')\n",
    "def check():\n",
    "    \n",
    "    return render_template('check_news.html')\n",
    "\n",
    "@app.route('/about')\n",
    "def about():\n",
    "    return render_template('about.html')\n",
    "\n",
    "@app.route('/contact')\n",
    "def contact():\n",
    "    \n",
    "    return render_template('contact.html')\n",
    "\n",
    "\n",
    "@app.route('/check', methods=['GET', 'POST'])\n",
    "def check_news():\n",
    "    if request.form.get('check'):\n",
    "        userAgent = ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3')\n",
    "        news_title = request.form.get('title')\n",
    "        global news_ss\n",
    "        news_ss.clear()\n",
    "        \n",
    "        non_base_links = []\n",
    "        search = news_title\n",
    "        googleText, googleLink = Google(search, userAgent)\n",
    "        \n",
    "        non_similar_titles=[]\n",
    "        googleLinks = googleLink\n",
    "        title_similarw = []\n",
    "        base_urls = []\n",
    "        for link in googleLinks:\n",
    "            parsed_uri = urlparse(link)\n",
    "            base_url = '{uri.scheme}://{uri.netloc}/'.format(uri=parsed_uri)\n",
    "            base_urls.append(base_url)\n",
    "            non_base_links.append(link)\n",
    "        \n",
    "        \n",
    "\n",
    "       \n",
    "\n",
    "        def cosine_similarity(a, b):\n",
    "            dot_product = np.dot(a, b)\n",
    "            norm_a = np.linalg.norm(a)\n",
    "            norm_b = np.linalg.norm(b)\n",
    "            return dot_product / (norm_a * norm_b)\n",
    "        \n",
    "        def jaccard_similarity(a, b):\n",
    "            seta = set(a)\n",
    "            setb = set(b)\n",
    "            intersection = seta.intersection(setb)\n",
    "            union = seta.union(setb)\n",
    "            return len(intersection)/ len(union)\n",
    "\n",
    "        # Find the cosine similarity between two titles using BERT\n",
    "        def cosine_similarity_using_bert(title1, title2):\n",
    "            # Preprocess the two titles using BERT preprocessor\n",
    "            title1_encoded = get_sentence_embeding([title1])[0]\n",
    "            title2_encoded = get_sentence_embeding([title2])[0]\n",
    "\n",
    "            # Compute the cosine similarity between the encoded titles\n",
    "            similarity = cosine_similarity(title1_encoded, title2_encoded)\n",
    "            return similarity\n",
    "\n",
    "        # The title which is used to compare the similarity between all the searched results\n",
    "        query_title = search\n",
    "\n",
    "        # The list of searched results\n",
    "        news_titles = googleText\n",
    "\n",
    "        # The list of trusted websites\n",
    "        trusted_websites = [\"https://ekantipur.com/\", \"https://annapurnapost.com/\", \"https://www.onlinekhabar.com/\",\n",
    "                            \"https://gorkhapatraonline.com/\", \"https://www.setopati.com/\",\n",
    "                            \"https://www.ratopati.com/category/main-news\", \"https://nagariknews.nagariknetwork.com/\",\n",
    "                            \"https://www.nayapatrikadaily.com/\", \"https://www.bbc.com/nepali\"]\n",
    "\n",
    "        # List to store the similar news title\n",
    "        similar_titles = []\n",
    "\n",
    "        # Iterating over each searched result to check the similarity\n",
    "        for i, title in enumerate(news_titles):\n",
    "            # Compute the cosine similarity between the query title and the current title\n",
    "            sim,jac_sim = cosine_similarity_using_bert(query_title, title),jaccard_similarity(query_title,title)\n",
    "            if sim >= 0.7 and jac_sim>=0.6:\n",
    "                title_similarw.append(title)\n",
    "                non_similar_titles.append(non_base_links[i])\n",
    "                # Check if the news is from trusted website\n",
    "                if base_urls[i] in trusted_websites:\n",
    "                    # Adding the news title to the list if the similarity is greater than 0.7 and news is from trusted website\n",
    "                    similar_titles.append(title)\n",
    "\n",
    "        # Printing the list of similar news titles\n",
    "        print(\"Similar titles: \", similar_titles)\n",
    "        \n",
    "        news_ss.extend(non_similar_titles)\n",
    "        \n",
    "        # List to store the trusted websites\n",
    "        trusted_websites_list = []\n",
    "\n",
    "        # Iterating over each news title\n",
    "        for title in similar_titles:\n",
    "            # Finding the index of the title in the list of searched results\n",
    "            index = news_titles.index(title)\n",
    "\n",
    "            # Adding the base URL to the list of trusted websites\n",
    "            trusted_websites_list.append(base_urls[index])\n",
    "\n",
    "        # Printing the list of trusted websites where the news is available\n",
    "        print(\"Trusted websites: \", trusted_websites_list)\n",
    "        global Prediction\n",
    "        if len(similar_titles) != 0 and len(trusted_websites_list) != 0:\n",
    "            \n",
    "            Prediction = \"True News\"\n",
    "            return render_template('check_news.html',Prediction = Prediction)\n",
    "        else:\n",
    "            \n",
    "            Prediction = \"News not found on trusted websites. Please enter the news description and click the predict button to run BERT model and to check authenticity of the news.\"\n",
    "            return render_template('check_news.html',Prediction = Prediction)\n",
    "\n",
    "        \n",
    "@app.route('/source', methods=['POST'])\n",
    "def source89():\n",
    "    if request.form.get('source'):\n",
    "        \n",
    "        return render_template('check_news.html',Prediction = Prediction,news_ss = news_ss)\n",
    "        \n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict_news():\n",
    "    if request.form.get('predict'):\n",
    "      news_description = request.form.get('description')\n",
    "      news_description1 = preprocessing(news_description)  \n",
    "    def get_prediction(news_description):\n",
    "        probability = loaded_model.predict([news_description])\n",
    "        \n",
    "        if probability[0][0] > 0.5:\n",
    "            Prediction = \"Fake News\"\n",
    "        else:\n",
    "            Prediction = \"True News\"\n",
    "        return render_template('check_news.html',Prediction = Prediction)\n",
    "        \n",
    "        \n",
    "    \n",
    "    return get_prediction(news_description)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53851294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfd3438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
